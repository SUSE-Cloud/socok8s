---
- name: Gather info about pods running
  command: "kubectl -n rook-ceph get pods -o jsonpath='{.items[*].metadata.name}'"
  register: _rook_ceph_pods
  changed_when: False

- name: Delete Ceph toolbox
  command: "kubectl delete -f toolbox.yaml"
  register: _rook_ceph_toolbox_delete
  args:
    chdir: "{{ ses_rook_k8s_manifests_dir }}"
  when: "_rook_ceph_pods.stdout is regex('rook-ceph-tools-.*')"

- name: Delete Ceph cluster
  command: "kubectl delete -f cluster.yaml"
  register: _rook_ceph_cluster_delete
  args:
    chdir: "{{ ses_rook_k8s_manifests_dir }}"
  when: "_rook_ceph_pods.stdout is regex('rook-ceph-mon-.*')"

- name: Delete rook operator
  command: "kubectl delete -f common.yaml -f csi/rbac/cephfs/ -f csi/rbac/rbd/ -f operator-with-csi.yaml"
  register: _rook_operator_delete
  args:
    chdir: "{{ ses_rook_k8s_manifests_dir }}"
  when: "_rook_ceph_pods.stdout is regex('rook-ceph-operator-.*')"

- name: Remove packages
  become: yes
  package:
    name: "{{ ses_rook_yaml_manifests_package }}"
    state: absent
  register: remove_packages
  until: remove_packages is success
  retries: 5
  delay: 2
  tags:
    - ses-rook-remove

- name: Delete SES repositories
  become: yes
  zypper_repository:
    name: "{{ item['key'] }}"
    repo: "{{ item['value'] }}"
    autorefresh: True
    auto_import_keys: yes
    state: absent
  loop: "{{ ses_rook_repos | dict2items }}"
  tags:
    - ses-rook-repository-delete

- name: Remove rook dir
  become: true
  delegate_to: "{{ item }}"
  file:
    state: absent
    path: "{{ ses_rook_config_path_in_caasp_workers }}"
  loop: "{{ groups['caasp-workers'] }}"

- debug:
    msg: |
      ses-rook has been removed. If you want to relaunch this role and redeploy ses-rook, you will need to
      clean up the disks, as they have been left with any test data+partitions that ceph created on them.
      We can't do this manually as this re-initialized the disks so its a bit dangerous to automate.
      You will need to run the following commands on each worker that had a disk used by ceph:

      sgdisk --zap-all $DISK
      ls /dev/mapper/ceph-* | xargs -I% -- dmsetup remove %
      rm -rf /dev/ceph-*

      This steps come from: https://rook.io/docs/rook/master/ceph-teardown.html
